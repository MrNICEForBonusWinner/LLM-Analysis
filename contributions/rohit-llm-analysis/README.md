# ğŸ§  LLM Comparison Analysis (Microsoft M365 PnP Contribution)

## ğŸ“Œ Problem Tackled
Compare multiple LLMs (e.g., GPT-4o, Claude, Gemini, WizardLM) on reasoning, consistency, instruction following, and response efficiency using a real benchmarking dataset.

## ğŸ§° Tools Used
- **Azure Blob Storage**: Data hosted and loaded
- **Jupyter + pandas**: Analysis and transformation
- **Power BI + Python Visuals**: Used for graphs
- **Seaborn/Matplotlib**: For heatmap, scatter, and bar charts

## ğŸ§ª Core Analysis Highlights
- GPT-4o and WizardLM topped **reasoning quality**
- Claude was fastest but GPT-4o offered **deeper insight**
- Some topics like DeFi showed **lower instruction-following**
- Enhanced context variants performed best
- Gemini had best **quality-to-speed** efficiency
- LLaMA was underused but **highly factual**

## ğŸ“Š Visual Insights Included
- Heatmap: Topic vs Info Density
- Scatter: Token count vs Factual Consistency
- Bar: Instruction Following by A/B Variant

## ğŸ¤ Why This Helps Microsoft
- Aids model evaluation and hallucination control
- Demonstrates community benchmarking potential
- Empowers prompt & UX tuning for M365 Copilot
- Promotes inclusive tech contributions

---

ğŸ“£ I'm proud to submit this as a **neurodiverse learner** passionate about AI fairness, transparency, and speed-to-quality tradeoffs.

